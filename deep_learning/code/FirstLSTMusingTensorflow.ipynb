{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PTB\n",
    "\n",
    "As part of reading this data, we need to convert the words present in the \\*.pb files to vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = tf.gfile.GFile(\"../datasets/dependency_treebank.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = f.read().replace(\"\\n\", \"<eos>\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_zip = np.load(\"../datasets/dependency_treebank.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.lib.npyio.NpzFile"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As\\tIN\\t7\\nan\\tDT\\t3\\nactor\\tNN\\t1\\n,\\t,\\t7\\nCharles\\tNNP\\t6\\nLane\\tNNP\\t7\\nis\\tVBZ\\t0\\nn't\\tRB\\t7\\nthe\\tDT\\t10\\ninheritor\\tNN\\t7\\nof\\tIN\\t10\\nCharlie\\tNNP\\t14\\nChaplin\\tNNP\\t14\\n's\\tPOS\\t15\\nspirit\\tNN\\t11\\n.\\t.\\t7\\n\\nSteve\\tNNP\\t2\\nMartin\\tNNP\\t3\\nhas\\tVBZ\\t0\\nalready\\tRB\\t3\\nlaid\\tVBN\\t3\\nhis\\tPRP$\\t7\\nclaim\\tNN\\t5\\nto\\tTO\\t7\\nthat\\tDT\\t8\\n.\\t.\\t3\\n\\nBut\\tCC\\t3\\nit\\tPRP\\t3\\nis\\tVBZ\\t0\\nMr.\\tNNP\\t5\\nLane\\tNNP\\t3\\n,\\t,\\t5\\nas\\tIN\\t5\\nmovie\\tNN\\t9\\ndirector\\tNN\\t13\\n,\\t,\\t13\\nproducer\\tNN\\t13\\nand\\tCC\\t13\\nwriter\\tNN\\t7\\n,\\t,\\t5\\nwho\\tWP\\t3\\nhas\\tVBZ\\t15\\nbeen\\tVBN\\t16\\nobsessed\\tVBN\\t17\\nwith\\tIN\\t18\\nrefitting\\tVBG\\t19\\nChaplin\\tNNP\\t22\\n's\\tPOS\\t24\\nLittle\\tNNP\\t24\\nTramp\\tNNP\\t20\\nin\\tIN\\t20\\na\\tDT\\t28\\ncontemporary\\tJJ\\t28\\nway\\tNN\\t25\\n.\\t.\\t3\\n\\nIn\\tIN\\t22\\n1976\\tCD\\t1\\n,\\t,\\t22\\nas\\tIN\\t22\\na\\tDT\\t7\\nfilm\\tNN\\t7\\nstudent\\tNN\\t4\\nat\\tIN\\t7\\nthe\\tDT\\t11\\nPurchase\\tNNP\\t11\\ncampus\\tNN\\t8\\nof\\tIN\\t11\\nthe\\tDT\\t15\\nState\\tNNP\\t15\\nUniversity\\tNNP\\t12\\nof\\tIN\\t15\\nNew\\tNNP\\t18\\nYork\\tNNP\\t16\\n,\\t,\\t22\\nMr.\\tNNP\\t21\\nLane\\tNNP\\t22\\nshot\\tVBD\\t0\\n``\\t``\\t33\\nA\\tDT\\t27\\nPlace\\tNNP\\t27\\nin\\tIN\\t27\\nTime\\tNNP\\t33\\n,\\t,\\t33\\n''\\t''\\t33\\na\\tDT\\t33\\n36-minute\\tJJ\\t33\\nblack-and-white\\tJJ\\t33\\nfilm\\tNN\\t22\\nabout\\tIN\\t33\\na\\tDT\\t37\\nsketch\\tNN\\t37\\nartist\\tNN\\t40\\n,\\t,\\t40\\na\\tDT\\t40\\nman\\tNN\\t34\\nof\\tIN\\t40\\nthe\\tDT\\t43\\nstreets\\tNNS\\t41\\n.\\t.\\t22\\n\\nNow\\tRB\\t9\\n,\\t,\\t9\\n13\\tCD\\t4\\nyears\\tNNS\\t5\\nlater\\tRBR\\t9\\n,\\t,\\t9\\nMr.\\tNNP\\t8\\nLane\\tNNP\\t9\\nhas\\tVBZ\\t0\\nrevived\\tVBN\\t9\\nhis\\tPRP$\\t12\\nArtist\\tNNP\\t10\\nin\\tIN\\t10\\na\\tDT\\t16\\nfull-length\\tJJ\\t16\\nmovie\\tNN\\t13\\ncalled\\tVBN\\t16\\n``\\t``\\t25\\nSidewalk\\tNNP\\t20\\nStories\\tNNP\\t25\\n,\\t,\\t25\\n''\\t''\\t25\\na\\tDT\\t25\\npoignant\\tJJ\\t25\\npiece\\tNN\\t17\\nof\\tIN\\t25\\nwork\\tNN\\t26\\nabout\\tIN\\t25\\na\\tDT\\t31\\nmodern-day\\tJJ\\t31\\ntramp\\tNN\\t28\\n.\\t.\\t9\\n\\nOf\\tIN\\t14\\ncourse\\tNN\\t1\\n,\\t,\\t14\\nif\\tIN\\t14\\nthe\\tDT\\t6\\nfilm\\tNN\\t7\\ncontained\\tVBD\\t4\\ndialogue\\tNN\\t7\\n,\\t,\\t14\\nMr.\\tNNP\\t12\\nLane\\tNNP\\t12\\n's\\tPOS\\t13\\nArtist\\tNNP\\t14\\nwould\\tMD\\t0\\nbe\\tVB\\t14\\ncalled\\tVBN\\t15\\na\\tDT\\t19\\nhomeless\\tJJ\\t19\\nperson\\tNN\\t16\\n.\\t.\\t14\\n\\nSo\\tRB\\t2\\nwould\\tMD\\t0\\nthe\\tDT\\t5\\nLittle\\tNNP\\t5\\nTramp\\tNNP\\t2\\n,\\t,\\t2\\nfor\\tIN\\t2\\nthat\\tDT\\t9\\nmatter\\tNN\\t7\\n.\\t.\\t2\\n\\nI\\tPRP\\t2\\nsay\\tVBP\\t0\\n``\\t``\\t2\\ncontained\\tVBD\\t2\\ndialogue\\tNN\\t4\\n''\\t''\\t2\\nbecause\\tIN\\t2\\n``\\t``\\t12\\nSidewalk\\tNNP\\t10\\nStories\\tNNP\\t12\\n''\\t''\\t12\\nis\\tVBZ\\t7\\nn't\\tRB\\t12\\nreally\\tRB\\t12\\nsilent\\tJJ\\t12\\nat\\tIN\\t12\\nall\\tDT\\t16\\n.\\t.\\t2\\n\\nComposer\\tNN\\t3\\nMarc\\tNNP\\t3\\nMarder\\tNNP\\t7\\n,\\t,\\t7\\na\\tDT\\t7\\ncollege\\tNN\\t7\\nfriend\\tNN\\t25\\nof\\tIN\\t7\\nMr.\\tNNP\\t11\\nLane\\tNNP\\t11\\n's\\tPOS\\t8\\nwho\\tWP\\t7\\nearns\\tVBZ\\t12\\nhis\\tPRP$\\t15\\nliving\\tNN\\t13\\nplaying\\tVBG\\t13\\nthe\\tDT\\t19\\ndouble\\tJJ\\t19\\nbass\\tNN\\t16\\nin\\tIN\\t16\\nclassical\\tJJ\\t23\\nmusic\\tNN\\t23\\nensembles\\tNNS\\t20\\n,\\t,\\t7\\nhas\\tVBZ\\t0\\nprepared\\tVBN\\t25\\nan\\tDT\\t31\\nexciting\\tJJ\\t31\\n,\\t,\\t31\\neclectic\\tJJ\\t31\\nscore\\tNN\\t26\\nthat\\tWDT\\t31\\ntells\\tVBZ\\t32\\nyou\\tPRP\\t33\\nwhat\\tWP\\t33\\nthe\\tDT\\t37\\ncharacters\\tNNS\\t38\\nare\\tVBP\\t35\\nthinking\\tVBG\\t38\\nand\\tCC\\t39\\nfeeling\\tVBG\\t39\\nfar\\tRB\\t33\\nmore\\tRBR\\t42\\nprecisely\\tRB\\t42\\nthan\\tIN\\t42\\nintertitles\\tNNS\\t50\\n,\\t,\\t50\\nor\\tCC\\t50\\neven\\tRB\\t50\\nwords\\tNNS\\t52\\n,\\t,\\t50\\nwould\\tMD\\t45\\n.\\t.\\t25\\n\\nMuch\\tRB\\t7\\nof\\tIN\\t1\\nMr.\\tNNP\\t5\\nLane\\tNNP\\t5\\n's\\tPOS\\t6\\nfilm\\tNN\\t2\\ntakes\\tVBZ\\t0\\na\\tDT\\t11\\nhighly\\tRB\\t10\\nromanticized\\tVBN\\t11\\nview\\tNN\\t7\\nof\\tIN\\t11\\nlife\\tNN\\t12\\non\\tIN\\t13\\nthe\\tDT\\t16\\nstreets\\tNNS\\t14\\n-LRB-\\t-LRB-\\t18\\nthough\\tIN\\t11\\nprobably\\tRB\\t18\\nno\\tDT\\t21\\nmore\\tRBR\\t19\\nromanticized\\tVBN\\t21\\nthan\\tIN\\t21\\nMr.\\tNNP\\t26\\nChaplin\\tNNP\\t26\\n's\\tPOS\\t27\\nnotion\\tNN\\t23\\nof\\tIN\\t27\\nthe\\tDT\\t30\\nTramp\\tNNP\\t28\\nas\\tIN\\t30\\nthe\\tDT\\t35\\ngood-hearted\\tJJ\\t35\\nfree\\tJJ\\t35\\nspirit\\tNN\\t31\\n-RRB-\\t-RRB-\\t18\\n.\\t.\\t7\\n\\nFilmed\\tVBN\\t20\\nin\\tIN\\t1\\nlovely\\tJJ\\t6\\nblack\\tNN\\t6\\nand\\tCC\\t6\\nwhite\\tNN\\t2\\nby\\tIN\\t1\\nBill\\tNNP\\t9\\nDill\\tNNP\\t7\\n,\\t,\\t20\\nthe\\tDT\\t14\\nNew\\tNNP\\t14\\nYork\\tNNP\\t14\\nstreets\\tNNS\\t20\\nof\\tIN\\t14\\n``\\t``\\t15\\nSidewalk\\tNNP\\t18\\nStories\\tNNP\\t15\\n''\\t''\\t15\\nseem\\tVBP\\t0\\nbenign\\tJJ\\t20\\n.\\t.\\t20\\n\\nOn\\tIN\\t7\\nWall\\tNNP\\t3\\nStreet\\tNNP\\t1\\nmen\\tNNS\\t6\\nand\\tCC\\t6\\nwomen\\tNNS\\t7\\nwalk\\tVBP\\t0\\nwith\\tIN\\t7\\ngreat\\tJJ\\t10\\npurpose\\tNN\\t8\\n,\\t,\\t7\\nnoticing\\tVBG\\t7\\none\\tCD\\t12\\nanother\\tDT\\t13\\nonly\\tRB\\t16\\nwhen\\tWRB\\t12\\nthey\\tPRP\\t18\\njostle\\tVBP\\t16\\nfor\\tIN\\t18\\ncabs\\tNNS\\t19\\n.\\t.\\t7\\n\\nThe\\tDT\\t2\\nArtist\\tNNP\\t3\\nhangs\\tVBZ\\t0\\nout\\tRP\\t3\\nin\\tIN\\t3\\nGreenwich\\tNNP\\t7\\nVillage\\tNNP\\t5\\n,\\t,\\t3\\non\\tIN\\t3\\na\\tDT\\t11\\nstrip\\tNN\\t9\\nof\\tIN\\t11\\nSixth\\tNNP\\t14\\nAvenue\\tNNP\\t12\\npopulated\\tVBN\\t11\\nby\\tIN\\t15\\njugglers\\tNNS\\t23\\n,\\t,\\t23\\nmagicians\\tNNS\\t23\\nand\\tCC\\t23\\nother\\tJJ\\t23\\ngood-natured\\tJJ\\t23\\nhustlers\\tNNS\\t16\\n.\\t.\\t3\\n\\n-LRB-\\t-LRB-\\t4\\nThis\\tDT\\t4\\nclearly\\tRB\\t4\\nis\\tVBZ\\t0\\nnot\\tRB\\t4\\nreal\\tJJ\\t7\\nlife\\tNN\\t4\\n:\\t:\\t4\\nno\\tDT\\t11\\ncrack\\tNN\\t11\\ndealers\\tNNS\\t23\\n,\\t,\\t23\\nno\\tDT\\t15\\ndead-eyed\\tJJ\\t15\\nmen\\tNNS\\t23\\nselling\\tVBG\\t15\\nfour-year-old\\tJJ\\t18\\ncopies\\tNNS\\t16\\nof\\tIN\\t18\\nCosmopolitan\\tNNP\\t19\\n,\\t,\\t23\\nno\\tDT\\t23\\none\\tPRP\\t4\\ncurled\\tVBD\\t23\\nup\\tRP\\t24\\nin\\tIN\\t24\\na\\tDT\\t29\\ncardboard\\tNN\\t29\\nbox\\tNN\\t26\\n.\\t.\\t4\\n-RRB-\\t-RRB-\\t4\\n\\nThe\\tDT\\t2\\nArtist\\tNNP\\t3\\nhas\\tVBZ\\t0\\nhis\\tPRP$\\t5\\nroutine\\tNN\\t3\\n.\\t.\\t3\\n\\nHe\\tPRP\\t2\\nspends\\tVBZ\\t0\\nhis\\tPRP$\\t4\\ndays\\tNNS\\t2\\nsketching\\tVBG\\t9\\npassers-by\\tNNS\\t5\\n,\\t,\\t9\\nor\\tCC\\t9\\ntrying\\tVBG\\t2\\nto\\tTO\\t9\\n.\\t.\\t2\\n\\nAt\\tIN\\t4\\nnight\\tNN\\t1\\nhe\\tPRP\\t4\\nreturns\\tVBZ\\t0\\nto\\tTO\\t4\\nthe\\tDT\\t8\\ncondemned\\tVBN\\t8\\nbuilding\\tNN\\t5\\nhe\\tPRP\\t10\\ncalls\\tVBZ\\t8\\nhome\\tNN\\t10\\n.\\t.\\t4\\n\\nHis\\tPRP$\\t2\\nlife\\tNN\\t13\\n,\\t,\\t2\\nincluding\\tVBG\\t2\\nhis\\tPRP$\\t6\\nskirmishes\\tNNS\\t4\\nwith\\tIN\\t6\\na\\tDT\\t11\\ncompeting\\tVBG\\t11\\nsketch\\tNN\\t11\\nartist\\tNN\\t7\\n,\\t,\\t2\\nseems\\tVBZ\\t0\\ncarefree\\tJJ\\t13\\n.\\t.\\t13\\n\\nHe\\tPRP\\t2\\nis\\tVBZ\\t0\\nhis\\tPRP$\\t5\\nown\\tJJ\\t5\\nman\\tNN\\t2\\n.\\t.\\t2\\n\\nThen\\tRB\\t22\\n,\\t,\\t22\\njust\\tRB\\t4\\nas\\tIN\\t22\\nthe\\tDT\\t6\\nTramp\\tNNP\\t7\\nis\\tVBZ\\t4\\ngiven\\tVBN\\t7\\na\\tDT\\t11\\nblind\\tJJ\\t11\\ngirl\\tNN\\t8\\nto\\tTO\\t13\\ncure\\tVB\\t11\\nin\\tIN\\t8\\n``\\t``\\t14\\nCity\\tNNP\\t17\\nLights\\tNNP\\t14\\n,\\t,\\t22\\n''\\t''\\t22\\nthe\\tDT\\t21\\nArtist\\tNNP\\t22\\nis\\tVBZ\\t0\\nput\\tVBN\\t22\\nin\\tIN\\t23\\ncharge\\tNN\\t24\\nof\\tIN\\t25\\nreturning\\tVBG\\t26\\na\\tDT\\t30\\ntwo-year-old\\tJJ\\t30\\nwaif\\tNN\\t27\\n-LRB-\\t-LRB-\\t33\\nNicole\\tNNP\\t33\\nAlysia\\tNNP\\t30\\n-RRB-\\t-RRB-\\t33\\n,\\t,\\t30\\nwhose\\tWP$\\t30\\nfather\\tNN\\t36\\nhas\\tVBZ\\t36\\nbeen\\tVBN\\t38\\nmurdered\\tVBN\\t39\\nby\\tIN\\t40\\nthugs\\tNNS\\t41\\n,\\t,\\t30\\nto\\tTO\\t27\\nher\\tPRP$\\t46\\nmother\\tNN\\t44\\n.\\t.\\t22\\n\\nThis\\tDT\\t3\\ncute\\tJJ\\t3\\nchild\\tNN\\t4\\nturns\\tVBZ\\t0\\nout\\tRP\\t4\\nto\\tTO\\t7\\nbe\\tVB\\t4\\na\\tDT\\t9\\nblessing\\tNN\\t12\\nand\\tCC\\t12\\na\\tDT\\t12\\ncurse\\tNN\\t7\\n.\\t.\\t4\\n\\nShe\\tPRP\\t2\\ngives\\tVBZ\\t0\\nthe\\tDT\\t4\\nArtist\\tNNP\\t2\\na\\tDT\\t6\\nsense\\tNN\\t2\\nof\\tIN\\t6\\npurpose\\tNN\\t7\\n,\\t,\\t2\\nbut\\tCC\\t2\\nalso\\tRB\\t12\\nalerts\\tVBZ\\t2\\nhim\\tPRP\\t12\\nto\\tTO\\t12\\nthe\\tDT\\t17\\nserious\\tJJ\\t17\\ninadequacy\\tNN\\t14\\nof\\tIN\\t17\\nhis\\tPRP$\\t21\\nvagrant\\tJJ\\t21\\nlife\\tNN\\t18\\n.\\t.\\t2\\n\\nThe\\tDT\\t2\\nbeds\\tNNS\\t7\\nat\\tIN\\t2\\nthe\\tDT\\t6\\nBowery\\tNNP\\t6\\nMission\\tNNP\\t3\\nseem\\tVBP\\t0\\nfar\\tJJ\\t7\\ndrearier\\tRBR\\t8\\nwhen\\tWRB\\t7\\nhe\\tPRP\\t12\\nhas\\tVBZ\\t10\\nto\\tTO\\t14\\ntuck\\tVB\\t12\\na\\tDT\\t17\\nlittle\\tJJ\\t17\\ngirl\\tNN\\t14\\ninto\\tIN\\t14\\none\\tCD\\t18\\nof\\tIN\\t19\\nthem\\tPRP\\t20\\nat\\tIN\\t14\\nnight\\tNN\\t22\\n.\\t.\\t7\\n\\nTo\\tTO\\t3\\nfurther\\tRBR\\t3\\nload\\tVB\\t9\\nthe\\tDT\\t5\\nstakes\\tNNS\\t3\\n,\\t,\\t9\\nMr.\\tNNP\\t8\\nLane\\tNNP\\t9\\ndreamed\\tVBD\\t0\\nup\\tRP\\t9\\na\\tDT\\t14\\nhighly\\tRB\\t13\\nimprobable\\tJJ\\t14\\nromance\\tNN\\t9\\nfor\\tIN\\t14\\nthe\\tDT\\t17\\nArtist\\tNNP\\t15\\n,\\t,\\t14\\nwith\\tIN\\t14\\na\\tDT\\t22\\nyoung\\tJJ\\t22\\nwoman\\tNN\\t19\\nwho\\tWP\\t31\\nowns\\tVBZ\\t23\\nher\\tPRP$\\t29\\nown\\tJJ\\t29\\nchildren\\tNNS\\t28\\n's\\tPOS\\t29\\nshop\\tNN\\t24\\nand\\tCC\\t31\\nwho\\tWP\\t22\\nlives\\tVBZ\\t31\\nin\\tIN\\t32\\nan\\tDT\\t38\\nexpensive\\tJJ\\t38\\nhigh-rise\\tJJ\\t38\\napartment\\tNN\\t38\\nbuilding\\tNN\\t33\\n.\\t.\\t9\\n\\nThis\\tDT\\t3\\nstory\\tNN\\t3\\nline\\tNN\\t4\\nmight\\tMD\\t0\\nresonate\\tVB\\t4\\nmore\\tRBR\\t7\\nstrongly\\tRB\\t5\\nif\\tIN\\t5\\nMr.\\tNNP\\t10\\nLane\\tNNP\\t11\\nhad\\tVBD\\t8\\nas\\tRB\\t15\\nstrong\\tJJ\\t15\\na\\tDT\\t15\\npresence\\tNN\\t11\\nin\\tIN\\t15\\nfront\\tNN\\t16\\nof\\tIN\\t17\\nthe\\tDT\\t20\\ncamera\\tNN\\t18\\nas\\tIN\\t15\\nhe\\tPRP\\t23\\ndoes\\tVBZ\\t21\\nbehind\\tIN\\t23\\nit\\tPRP\\t24\\n.\\t.\\t4\\n\\nMr.\\tNNP\\t3\\nLane\\tNNP\\t3\\n's\\tPOS\\t5\\nfinal\\tJJ\\t5\\npurpose\\tNN\\t6\\nis\\tVBZ\\t0\\nn't\\tRB\\t6\\nto\\tTO\\t9\\nglamorize\\tVB\\t6\\nthe\\tDT\\t12\\nArtist\\tNNP\\t12\\n's\\tPOS\\t14\\nvagabond\\tNN\\t14\\nexistence\\tNN\\t9\\n.\\t.\\t6\\n\\nHe\\tPRP\\t2\\nhas\\tVBZ\\t12\\na\\tDT\\t4\\npoint\\tNN\\t2\\nhe\\tPRP\\t6\\nwants\\tVBZ\\t4\\nto\\tTO\\t8\\nmake\\tVB\\t6\\n,\\t,\\t12\\nand\\tCC\\t12\\nhe\\tPRP\\t12\\nmakes\\tVBZ\\t0\\nit\\tPRP\\t12\\n,\\t,\\t12\\nwith\\tIN\\t12\\na\\tDT\\t18\\ngreat\\tJJ\\t18\\ndeal\\tNN\\t15\\nof\\tIN\\t18\\nforce\\tNN\\t19\\n.\\t.\\t12\\n\\nThe\\tDT\\t2\\nmovie\\tNN\\t3\\nends\\tVBZ\\t16\\nwith\\tIN\\t3\\nsound\\tNN\\t8\\n,\\t,\\t8\\nthe\\tDT\\t8\\nsound\\tNN\\t4\\nof\\tIN\\t8\\nstreet\\tNN\\t11\\npeople\\tNNS\\t12\\ntalking\\tVBG\\t9\\n,\\t,\\t8\\nand\\tCC\\t16\\nthere\\tEX\\t16\\nis\\tVBZ\\t0\\nn't\\tRB\\t16\\nanything\\tNN\\t16\\nwhimsical\\tJJ\\t21\\nor\\tCC\\t21\\nenviable\\tJJ\\t18\\nin\\tIN\\t16\\nthose\\tDT\\t27\\nrough\\tJJ\\t27\\n,\\t,\\t27\\nbeaten\\tJJ\\t27\\nvoices\\tNNS\\t22\\n.\\t.\\t16\\n\\nThe\\tDT\\t6\\nFrench\\tJJ\\t6\\nfilm\\tNN\\t6\\nmaker\\tNN\\t6\\nClaude\\tNNP\\t6\\nChabrol\\tNNP\\t7\\nhas\\tVBZ\\t0\\nmanaged\\tVBN\\t7\\nanother\\tDT\\t10\\nkind\\tNN\\t8\\nof\\tIN\\t10\\nweird\\tJJ\\t13\\nachievement\\tNN\\t11\\nwith\\tIN\\t8\\nhis\\tPRP$\\t17\\n``\\t``\\t17\\nStory\\tNNP\\t14\\nof\\tIN\\t17\\nWomen\\tNNP\\t18\\n.\\t.\\t7\\n''\\t''\\t7\\n\\nHe\\tPRP\\t2\\nhas\\tVBZ\\t0\\nmade\\tVBN\\t2\\na\\tDT\\t8\\nharsh\\tJJ\\t8\\n,\\t,\\t8\\nbrilliant\\tJJ\\t8\\npicture\\tNN\\t3\\n--\\t:\\t10\\none\\tCD\\t8\\nthat\\tWDT\\t10\\n's\\tVBZ\\t11\\ncaptivating\\tJJ\\t12\\n--\\t:\\t10\\nabout\\tIN\\t8\\na\\tDT\\t17\\ncharacter\\tNN\\t15\\nwho\\tWP\\t17\\n,\\t,\\t18\\nviewed\\tVBN\\t27\\nfrom\\tIN\\t20\\nthe\\tDT\\t25\\nmost\\tRBS\\t24\\nsympathetic\\tJJ\\t25\\nangle\\tNN\\t21\\n,\\t,\\t27\\nwould\\tMD\\t18\\nseem\\tVB\\t27\\ndisagreeable\\tJJ\\t28\\n.\\t.\\t2\\n\\nYet\\tRB\\t8\\nthis\\tDT\\t3\\nwoman\\tNN\\t6\\n,\\t,\\t6\\nMarie-Louise\\tNNP\\t6\\nGiraud\\tNNP\\t8\\n,\\t,\\t6\\ncarries\\tVBZ\\t0\\nhistorical\\tJJ\\t10\\nsignificance\\tNN\\t8\\n,\\t,\\t8\\nboth\\tDT\\t8\\nas\\tIN\\t12\\none\\tCD\\t13\\nof\\tIN\\t14\\nthe\\tDT\\t18\\nlast\\tJJ\\t18\\nwomen\\tNNS\\t15\\nto\\tTO\\t20\\nbe\\tVB\\t18\\nexecuted\\tVBN\\t20\\nin\\tIN\\t21\\nFrance\\tNNP\\t22\\nand\\tCC\\t12\\nas\\tIN\\t12\\na\\tDT\\t27\\nsymbol\\tNN\\t25\\nof\\tIN\\t27\\nthe\\tDT\\t32\\nVichy\\tNNP\\t32\\ngovernment\\tNN\\t32\\n's\\tPOS\\t33\\nhypocrisy\\tNN\\t28\\n.\\t.\\t8\\n\\nWhile\\tIN\\t24\\nVichy\\tNNP\\t3\\ncollaborated\\tVBD\\t1\\nwith\\tIN\\t3\\nthe\\tDT\\t6\\nGermans\\tNNS\\t4\\nduring\\tIN\\t3\\nWorld\\tNNP\\t10\\nWar\\tNNP\\t10\\nII\\tNNP\\t7\\nin\\tIN\\t3\\nthe\\tDT\\t13\\ndeaths\\tNNS\\t11\\nof\\tIN\\t13\\nthousands\\tNNS\\t14\\nof\\tIN\\t15\\nResistance\\tNNP\\t18\\nfighters\\tNNS\\t20\\nand\\tCC\\t20\\nJews\\tNNS\\t16\\n,\\t,\\t24\\nits\\tPRP$\\t23\\nofficials\\tNNS\\t24\\nneeded\\tVBD\\t0\\na\\tDT\\t28\\ndiversionary\\tJJ\\t28\\nsymbolic\\tJJ\\t28\\ntraitor\\tNN\\t24\\n.\\t.\\t24\\n\\nMarie-Louise\\tNNP\\t5\\n,\\t,\\t5\\na\\tDT\\t5\\nsmall-time\\tJJ\\t5\\nabortionist\\tNN\\t7\\n,\\t,\\t5\\nwas\\tVBD\\t0\\ntheir\\tPRP$\\t9\\nwoman\\tNN\\t7\\n.\\t.\\t7\\n\\nShe\\tPRP\\t2\\nbecame\\tVBD\\t0\\nan\\tDT\\t4\\nabortionist\\tNN\\t2\\naccidentally\\tRB\\t2\\n,\\t,\\t2\\nand\\tCC\\t2\\ncontinued\\tVBD\\t2\\nbecause\\tIN\\t8\\nit\\tPRP\\t11\\nenabled\\tVBD\\t9\\nher\\tPRP\\t14\\nto\\tTO\\t14\\nbuy\\tVB\\t11\\njam\\tNN\\t21\\n,\\t,\\t21\\ncocoa\\tNN\\t21\\nand\\tCC\\t21\\nother\\tJJ\\t21\\nwar-rationed\\tJJ\\t21\\ngoodies\\tNNS\\t14\\n.\\t.\\t2\\n\\nShe\\tPRP\\t2\\nwas\\tVBD\\t0\\nuntrained\\tJJ\\t2\\nand\\tCC\\t2\\n,\\t,\\t2\\nin\\tIN\\t10\\none\\tCD\\t9\\nbotched\\tJJ\\t9\\njob\\tNN\\t6\\nkilled\\tVBD\\t2\\na\\tDT\\t12\\nclient\\tNN\\t10\\n.\\t.\\t2\\n\\nHer\\tPRP$\\t2\\nremorse\\tNN\\t3\\nwas\\tVBD\\t0\\nshallow\\tJJ\\t6\\nand\\tCC\\t6\\nbrief\\tJJ\\t3\\n.\\t.\\t3\\n\\nAlthough\\tIN\\t21\\nshe\\tPRP\\t3\\nwas\\tVBD\\t1\\nkind\\tJJ\\t6\\nand\\tCC\\t6\\nplayful\\tJJ\\t3\\nto\\tTO\\t6\\nher\\tPRP$\\t9\\nchildren\\tNNS\\t7\\n,\\t,\\t21\\nshe\\tPRP\\t12\\nwas\\tVBD\\t21\\ndreadful\\tJJ\\t12\\nto\\tTO\\t13\\nher\\tPRP$\\t17\\nwar-damaged\\tJJ\\t17\\nhusband\\tNN\\t14\\n;\\t:\\t21\\nshe\\tPRP\\t21\\nopenly\\tRB\\t21\\nbrought\\tVBD\\t0\\nher\\tPRP$\\t23\\nlover\\tNN\\t21\\ninto\\tIN\\t21\\ntheir\\tPRP$\\t26\\nhome\\tNN\\t24\\n.\\t.\\t21\\n\\nAs\\tIN\\t25\\npresented\\tVBN\\t1\\nby\\tIN\\t2\\nMr.\\tNNP\\t5\\nChabrol\\tNNP\\t3\\n,\\t,\\t2\\nand\\tCC\\t2\\nplayed\\tVBN\\t2\\nwith\\tIN\\t8\\nthin-lipped\\tJJ\\t11\\nintensity\\tNN\\t9\\nby\\tIN\\t8\\nIsabelle\\tNNP\\t14\\nHuppert\\tNNP\\t12\\n,\\t,\\t25\\nMarie-Louise\\tNNP\\t25\\n-LRB-\\t-LRB-\\t18\\ncalled\\tVBN\\t16\\nMarie\\tNNP\\t20\\nLatour\\tNNP\\t18\\nin\\tIN\\t18\\nthe\\tDT\\t23\\nfilm\\tNN\\t21\\n-RRB-\\t-RRB-\\t18\\nwas\\tVBD\\t0\\nnot\\tRB\\t25\\na\\tDT\\t29\\nnice\\tJJ\\t29\\nperson\\tNN\\t25\\n.\\t.\\t25\\n\\nBut\\tCC\\t3\\nshe\\tPRP\\t3\\ndid\\tVBD\\t0\\nn't\\tRB\\t3\\ndeserve\\tVB\\t3\\nto\\tTO\\t7\\nhave\\tVB\\t5\\nher\\tPRP$\\t9\\nhead\\tNN\\t10\\nchopped\\tVBN\\t7\\noff\\tRP\\t10\\n.\\t.\\t3\\n\\nThere\\tEX\\t2\\nis\\tVBZ\\t0\\nvery\\tRB\\t4\\nlittle\\tJJ\\t2\\nto\\tTO\\t6\\nrecommend\\tVB\\t4\\n``\\t``\\t14\\nOld\\tNNP\\t9\\nGringo\\tNNP\\t14\\n,\\t,\\t14\\n''\\t''\\t14\\na\\tDT\\t14\\nconfused\\tJJ\\t14\\nrendering\\tNN\\t6\\nof\\tIN\\t14\\nthe\\tDT\\t19\\nCarlos\\tNNP\\t19\\nFuentes\\tNNP\\t19\\nnovel\\tNN\\t15\\nof\\tIN\\t19\\nthe\\tDT\\t23\\nMexican\\tNNP\\t23\\nRevolution\\tNNP\\t20\\n.\\t.\\t2\\n\\nMost\\tJJS\\t5\\nof\\tIN\\t1\\nthe\\tDT\\t4\\npicture\\tNN\\t2\\nis\\tVBZ\\t0\\ntaken\\tVBN\\t5\\nup\\tRP\\t6\\nwith\\tIN\\t6\\nendless\\tJJ\\t10\\nscenes\\tNNS\\t8\\nof\\tIN\\t10\\nmany\\tJJ\\t13\\npeople\\tNNS\\t15\\neither\\tCC\\t15\\nfighting\\tVBG\\t11\\nor\\tCC\\t15\\neating\\tVBG\\t15\\nand\\tCC\\t17\\ndrinking\\tVBG\\t17\\nto\\tTO\\t21\\ncelebrate\\tVB\\t17\\nvictory\\tNN\\t21\\n.\\t.\\t5\\n\\nI\\tPRP\\t2\\nmention\\tVBP\\t0\\nthe\\tDT\\t4\\npicture\\tNN\\t2\\nonly\\tRB\\t6\\nbecause\\tIN\\t2\\nmany\\tJJ\\t9\\nbad\\tJJ\\t9\\nmovies\\tNNS\\t10\\nhave\\tVBP\\t18\\na\\tDT\\t13\\nbright\\tJJ\\t13\\nspot\\tNN\\t10\\n,\\t,\\t18\\nand\\tCC\\t18\\nthis\\tDT\\t17\\none\\tCD\\t18\\nhas\\tVBZ\\t6\\nGregory\\tNNP\\t20\\nPeck\\tNNP\\t18\\n,\\t,\\t20\\nin\\tIN\\t20\\na\\tDT\\t28\\nmarvelously\\tRB\\t27\\nloose\\tJJ\\t27\\nand\\tCC\\t27\\nenergetic\\tJJ\\t28\\nportrayal\\tNN\\t22\\nof\\tIN\\t28\\nan\\tDT\\t32\\nold\\tJJ\\t32\\nman\\tNN\\t29\\nwho\\tWP\\t32\\nwants\\tVBZ\\t33\\nto\\tTO\\t36\\ndie\\tVB\\t34\\nthe\\tDT\\t38\\nway\\tNN\\t36\\nhe\\tPRP\\t40\\nwants\\tVBZ\\t38\\nto\\tTO\\t42\\ndie\\tVB\\t40\\n.\\t.\\t2\\n\\nVideo\\tNNP\\t2\\nTip\\tNNP\\t0\\n:\\t:\\t2\\nBefore\\tIN\\t11\\nseeing\\tVBG\\t4\\n``\\t``\\t5\\nSidewalk\\tNNP\\t8\\nStories\\tNNP\\t5\\n,\\t,\\t11\\n''\\t''\\t11\\ntake\\tVB\\t2\\na\\tDT\\t13\\nlook\\tNN\\t11\\nat\\tIN\\t13\\n``\\t``\\t22\\nCity\\tNNP\\t17\\nLights\\tNNP\\t22\\n,\\t,\\t22\\n''\\t''\\t22\\nChaplin\\tNNP\\t21\\n's\\tPOS\\t22\\nTramp\\tNNP\\t14\\nat\\tIN\\t22\\nhis\\tPRP$\\t25\\nfinest\\tJJS\\t23\\n.\\t.\\t2\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_zip[np_zip.files[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = \"../datasets/dependency_treebank.zip\"\n",
    "\n",
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = ptb_raw_data(\"../datasets/dependency_treebank/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../datasets/dependency_treebank/\"\n",
    "train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "test_path = os.path.join(data_path, \"ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    data = f.read().replace(\"\\n\", \"<eos>\").split() # Extracting words from the training file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function split:\n",
      "\n",
      "split(...)\n",
      "    S.split([sep [,maxsplit]]) -> list of strings\n",
      "    \n",
      "    Return a list of the words in the string S, using sep as the\n",
      "    delimiter string.  If maxsplit is given, at most maxsplit\n",
      "    splits are done. If sep is not specified or is None, any\n",
      "    whitespace string is a separator and empty strings are removed\n",
      "    from the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(\"\".split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter(data) # Counts number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Counter in module collections:\n",
      "\n",
      "class Counter(__builtin__.dict)\n",
      " |  Dict subclass for counting hashable items.  Sometimes called a bag\n",
      " |  or multiset.  Elements are stored as dictionary keys and their counts\n",
      " |  are stored as dictionary values.\n",
      " |  \n",
      " |  >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n",
      " |  \n",
      " |  >>> c.most_common(3)                # three most common elements\n",
      " |  [('a', 5), ('b', 4), ('c', 3)]\n",
      " |  >>> sorted(c)                       # list all unique elements\n",
      " |  ['a', 'b', 'c', 'd', 'e']\n",
      " |  >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n",
      " |  'aaaaabbbbcccdde'\n",
      " |  >>> sum(c.values())                 # total of all counts\n",
      " |  15\n",
      " |  \n",
      " |  >>> c['a']                          # count of letter 'a'\n",
      " |  5\n",
      " |  >>> for elem in 'shazam':           # update counts from an iterable\n",
      " |  ...     c[elem] += 1                # by adding 1 to each element's count\n",
      " |  >>> c['a']                          # now there are seven 'a'\n",
      " |  7\n",
      " |  >>> del c['b']                      # remove all 'b'\n",
      " |  >>> c['b']                          # now there are zero 'b'\n",
      " |  0\n",
      " |  \n",
      " |  >>> d = Counter('simsalabim')       # make another counter\n",
      " |  >>> c.update(d)                     # add in the second counter\n",
      " |  >>> c['a']                          # now there are nine 'a'\n",
      " |  9\n",
      " |  \n",
      " |  >>> c.clear()                       # empty the counter\n",
      " |  >>> c\n",
      " |  Counter()\n",
      " |  \n",
      " |  Note:  If a count is set to zero or reduced to zero, it will remain\n",
      " |  in the counter until the entry is deleted or the counter is cleared:\n",
      " |  \n",
      " |  >>> c = Counter('aaabbc')\n",
      " |  >>> c['b'] -= 2                     # reduce the count of 'b' by two\n",
      " |  >>> c.most_common()                 # 'b' is still in, but its count is zero\n",
      " |  [('a', 3), ('c', 1), ('b', 0)]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Counter\n",
      " |      __builtin__.dict\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two counters.\n",
      " |      \n",
      " |      >>> Counter('abbb') + Counter('bcc')\n",
      " |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |      \n",
      " |      >>> Counter('abbb') & Counter('bcc')\n",
      " |      Counter({'b': 1})\n",
      " |  \n",
      " |  __delitem__(self, elem)\n",
      " |      Like dict.__delitem__() but does not raise KeyError for missing values.\n",
      " |  \n",
      " |  __init__(*args, **kwds)\n",
      " |      Create a new, empty Counter object.  And if given, count elements\n",
      " |      from an input iterable.  Or, initialize the count from another mapping\n",
      " |      of elements to their counts.\n",
      " |      \n",
      " |      >>> c = Counter()                           # a new, empty counter\n",
      " |      >>> c = Counter('gallahad')                 # a new counter from an iterable\n",
      " |      >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n",
      " |      >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n",
      " |  \n",
      " |  __missing__(self, key)\n",
      " |      The count of elements not in the Counter is zero.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |      \n",
      " |      >>> Counter('abbb') | Counter('bcc')\n",
      " |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |      \n",
      " |      >>> Counter('abbbc') - Counter('bccd')\n",
      " |      Counter({'b': 2, 'a': 1})\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Return a shallow copy.\n",
      " |  \n",
      " |  elements(self)\n",
      " |      Iterator over elements repeating each as many times as its count.\n",
      " |      \n",
      " |      >>> c = Counter('ABCABC')\n",
      " |      >>> sorted(c.elements())\n",
      " |      ['A', 'A', 'B', 'B', 'C', 'C']\n",
      " |      \n",
      " |      # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
      " |      >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
      " |      >>> product = 1\n",
      " |      >>> for factor in prime_factors.elements():     # loop over factors\n",
      " |      ...     product *= factor                       # and multiply them\n",
      " |      >>> product\n",
      " |      1836\n",
      " |      \n",
      " |      Note, if an element's count has been set to zero or is a negative\n",
      " |      number, elements() will ignore it.\n",
      " |  \n",
      " |  most_common(self, n=None)\n",
      " |      List the n most common elements and their counts from the most\n",
      " |      common to the least.  If n is None, then list all element counts.\n",
      " |      \n",
      " |      >>> Counter('abcdeabcdabcaba').most_common(3)\n",
      " |      [('a', 5), ('b', 4), ('c', 3)]\n",
      " |  \n",
      " |  subtract(*args, **kwds)\n",
      " |      Like dict.update() but subtracts counts instead of replacing them.\n",
      " |      Counts can be reduced below zero.  Both the inputs and outputs are\n",
      " |      allowed to contain zero and negative counts.\n",
      " |      \n",
      " |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      " |      \n",
      " |      >>> c = Counter('which')\n",
      " |      >>> c.subtract('witch')             # subtract elements from another iterable\n",
      " |      >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
      " |      >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
      " |      0\n",
      " |      >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
      " |      -1\n",
      " |  \n",
      " |  update(*args, **kwds)\n",
      " |      Like dict.update() but add counts instead of replacing them.\n",
      " |      \n",
      " |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      " |      \n",
      " |      >>> c = Counter('which')\n",
      " |      >>> c.update('witch')           # add elements from another iterable\n",
      " |      >>> d = Counter('watch')\n",
      " |      >>> c.update(d)                 # add elements from another counter\n",
      " |      >>> c['h']                      # four 'h' in which, witch, and watch\n",
      " |      4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromkeys(cls, iterable, v=None) from __builtin__.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from __builtin__.dict:\n",
      " |  \n",
      " |  __cmp__(...)\n",
      " |      x.__cmp__(y) <==> cmp(x,y)\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      D.__contains__(k) -> True if D has a key k, else False\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __getattribute__(...)\n",
      " |      x.__getattribute__('name') <==> x.name\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      x.__iter__() <==> iter(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      x.__len__() <==> len(x)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __setitem__(...)\n",
      " |      x.__setitem__(i, y) <==> x[i]=y\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  has_key(...)\n",
      " |      D.has_key(k) -> True if D has a key k, else False\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
      " |  \n",
      " |  iteritems(...)\n",
      " |      D.iteritems() -> an iterator over the (key, value) items of D\n",
      " |  \n",
      " |  iterkeys(...)\n",
      " |      D.iterkeys() -> an iterator over the keys of D\n",
      " |  \n",
      " |  itervalues(...)\n",
      " |      D.itervalues() -> an iterator over the values of D\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> list of D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> list of D's values\n",
      " |  \n",
      " |  viewitems(...)\n",
      " |      D.viewitems() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  viewkeys(...)\n",
      " |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  viewvalues(...)\n",
      " |      D.viewvalues() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from __builtin__.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x87', 1686)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(count_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 5299),\n",
       " ('s', 4335),\n",
       " ('<', 3405),\n",
       " ('o', 3333),\n",
       " ('>', 3258),\n",
       " ('n', 3053),\n",
       " ('\\x03', 2885),\n",
       " ('d', 2837),\n",
       " ('p', 2567)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pairs[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words, _ = list(zip(*count_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id = dict(zip(words, range(len(words))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x0c', 202)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_to_id.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = [word_to_id[word] for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 7, 37, 3, 1, 4, 2, 5, 0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Example / benchmark for building a PTB LSTM model.\n",
    "\n",
    "Trains the model described in:\n",
    "(Zaremba, et. al.) Recurrent Neural Network Regularization\n",
    "http://arxiv.org/abs/1409.2329\n",
    "\n",
    "There are 3 supported model configurations:\n",
    "===========================================\n",
    "| config | epochs | train | valid  | test\n",
    "===========================================\n",
    "| small  | 13     | 37.99 | 121.39 | 115.91\n",
    "| medium | 39     | 48.45 |  86.16 |  82.07\n",
    "| large  | 55     | 37.87 |  82.62 |  78.29\n",
    "The exact results may vary depending on the random initialization.\n",
    "\n",
    "The hyperparameters used in the model:\n",
    "- init_scale - the initial scale of the weights\n",
    "- learning_rate - the initial value of the learning rate\n",
    "- max_grad_norm - the maximum permissible norm of the gradient\n",
    "- num_layers - the number of LSTM layers\n",
    "- num_steps - the number of unrolled steps of LSTM\n",
    "- hidden_size - the number of LSTM units\n",
    "- max_epoch - the number of epochs trained with the initial learning rate\n",
    "- max_max_epoch - the total number of epochs for training\n",
    "- keep_prob - the probability of keeping weights in the dropout layer\n",
    "- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "- batch_size - the batch size\n",
    "\n",
    "The data required for this example is in the data/ dir of the\n",
    "PTB dataset from Tomas Mikolov's webpage:\n",
    "\n",
    "$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "$ tar xvf simple-examples.tgz\n",
    "\n",
    "To run:\n",
    "\n",
    "$ python ptb_word_lm.py --data_path=simple-examples/data/\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"small\",\n",
    "    \"A type of model. Possible options are: small, medium, large.\")\n",
    "flags.DEFINE_string(\"data_path\", None, \"data_path\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class PTBModel(object):\n",
    "  \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, config):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "\n",
    "    self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "    # Slightly better results can be obtained with forget gate biases\n",
    "    # initialized to 1 but the hyperparameters of the model would need to be\n",
    "    # different than reported in the paper.\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "          lstm_cell, output_keep_prob=config.keep_prob)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "\n",
    "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "      inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    # Simplified version of tensorflow.models.rnn.rnn.py's rnn().\n",
    "    # This builds an unrolled LSTM for tutorial purposes only.\n",
    "    # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "    #\n",
    "    # The alternative version of the code below is:\n",
    "    #\n",
    "    # from tensorflow.models.rnn import rnn\n",
    "    # inputs = [tf.squeeze(input_, [1])\n",
    "    #           for input_ in tf.split(1, num_steps, inputs)]\n",
    "    # outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n",
    "    outputs = []\n",
    "    state = self._initial_state\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "      for time_step in range(num_steps):\n",
    "        if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "        outputs.append(cell_output)\n",
    "\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(self._targets, [-1])],\n",
    "        [tf.ones([batch_size * num_steps])])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = state\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "  @property\n",
    "  def input_data(self):\n",
    "    return self._input_data\n",
    "\n",
    "  @property\n",
    "  def targets(self):\n",
    "    return self._targets\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "\n",
    "\n",
    "class SmallConfig(object):\n",
    "  \"\"\"Small config.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 20\n",
    "  hidden_size = 200\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "  \"\"\"Large config.\"\"\"\n",
    "  init_scale = 0.04\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 10\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 1500\n",
    "  max_epoch = 14\n",
    "  max_max_epoch = 55\n",
    "  keep_prob = 0.35\n",
    "  lr_decay = 1 / 1.15\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 1\n",
    "  num_steps = 2\n",
    "  hidden_size = 2\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "  \"\"\"Runs the model on the given data.\"\"\"\n",
    "  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  state = m.initial_state.eval()\n",
    "  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n",
    "                                                    m.num_steps)):\n",
    "    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                 {m.input_data: x,\n",
    "                                  m.targets: y,\n",
    "                                  m.initial_state: state})\n",
    "    costs += cost\n",
    "    iters += m.num_steps\n",
    "\n",
    "    if verbose and step % (epoch_size // 10) == 10:\n",
    "      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "             iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "  if FLAGS.model == \"small\":\n",
    "    return SmallConfig()\n",
    "  elif FLAGS.model == \"medium\":\n",
    "    return MediumConfig()\n",
    "  elif FLAGS.model == \"large\":\n",
    "    return LargeConfig()\n",
    "  elif FLAGS.model == \"test\":\n",
    "    return TestConfig()\n",
    "  else:\n",
    "    raise ValueError(\"Invalid model: %s\", FLAGS.model)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if not FLAGS.data_path:\n",
    "    raise ValueError(\"Must set --data_path to PTB data directory\")\n",
    "\n",
    "  raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "  train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "  config = get_config()\n",
    "  eval_config = get_config()\n",
    "  eval_config.batch_size = 1\n",
    "  eval_config.num_steps = 1\n",
    "\n",
    "  with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "      m = PTBModel(is_training=True, config=config)\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "      mvalid = PTBModel(is_training=False, config=config)\n",
    "      mtest = PTBModel(is_training=False, config=eval_config)\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(config.max_max_epoch):\n",
    "      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "      m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "      print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "      train_perplexity = run_epoch(session, m, train_data, m.train_op,\n",
    "                                   verbose=True)\n",
    "      print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "      valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "      print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "count() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3dc4872ec659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: count() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
